{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Simple tutorial for Skip-gram model\n",
    "\n",
    "Tasks:\n",
    "1. Implement your_preprocessing() function which receives raw_text as input\n",
    "2. Implement make_all_word_pairs()\n",
    "3. Implement save_word_vectors() to save weghts vectors for given vocabulary\n",
    "4. Compare several optimizers, including Adam and Adadelta and compare the speech of convergence\n",
    "5. Implement CBOW model in a similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "import zipfile\n",
    "import collections\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class skipgram(nn.Module):\n",
    "    \n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "      \n",
    "    super(skipgram, self).__init__()\n",
    "    self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)   \n",
    "    self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True) \n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.init_emb()\n",
    "    \n",
    "  def init_emb(self):\n",
    "    initrange = 0.5 / self.embedding_dim\n",
    "    self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "    \n",
    "  def forward(self, u_pos, v_pos, v_neg, batch_size):\n",
    "\n",
    "    embed_u = self.u_embeddings(u_pos)\n",
    "    embed_v = self.v_embeddings(v_pos)\n",
    "\n",
    "    score  = torch.mul(embed_u, embed_v)\n",
    "    score = torch.sum(score, dim=1)\n",
    "    log_target = F.logsigmoid(score).squeeze()\n",
    "    \n",
    "    neg_embed_v = self.v_embeddings(v_neg)\n",
    "    \n",
    "    neg_score = torch.bmm(neg_embed_v, embed_u.unsqueeze(2)).squeeze()\n",
    "    neg_score = torch.sum(neg_score, dim=1)\n",
    "    sum_log_sampled = F.logsigmoid(-neg_score).squeeze()\n",
    "    loss = log_target + sum_log_sampled\n",
    "\n",
    "    return -loss.sum()/batch_size\n",
    "\n",
    "  def save_word_vectors(self, file_name):\n",
    "      \n",
    "      ''' Save for each word its vector to the file_name\n",
    "          E.g   word1 vector1\n",
    "                word2 vector2\n",
    "                ....\n",
    "      '''\n",
    "      \n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \n",
    "  def __init__(self, input_file='Dataset2.txt', vocabulary_size=100000, embedding_dim=200, epoch_num=20, batch_size=16, windows_size=5, neg_sample_num=10):\n",
    "    \n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.windows_size = windows_size\n",
    "    self.vocabulary_size = vocabulary_size\n",
    "    self.batch_size = batch_size\n",
    "    self.epoch_num = epoch_num\n",
    "    self.neg_sample_num = neg_sample_num\n",
    "    self.context_size = 2\n",
    "    self.input_file = input_file\n",
    "    \n",
    "    self.raw_text = open(self.input_file, 'r').read()\n",
    "    self.preprocessed_text = self.your_preprocessing(self.raw_text)\n",
    "    self.word_to_ix = { word: i for i, word in enumerate(set(self.preprocessed_text))}\n",
    "\n",
    "  def your_preprocessing(self, raw_text):\n",
    "      # Please define here your preprocessing\n",
    "      return raw_text\n",
    "    \n",
    "  def make_all_word_pairs(self): \n",
    "      \n",
    "      ''' Returns array pos_u, pos_v and neg_v\n",
    "          pos_u = (N*2*self.window_size, )\n",
    "          pos_v = (N*2*self.window_size, )\n",
    "          neg_v = (N*2*self.window_size, self.neg_sample_num),\n",
    "          \n",
    "          where N ~ num of word tokens\n",
    "          the values of arrays are word indices from self.word_to_ix\n",
    "      '''\n",
    "      \n",
    "      # Write your code here\n",
    "      \n",
    "      return np.array(self.pos_u, dtype=np.int64), np.array(self.pos_v, dtype=np.int64), np.array(self.neg_v, dtype=np.int64)\n",
    "       \n",
    "  def train(self):\n",
    "        model = skipgram(self.vocabulary_size, self.embedding_dim)\n",
    "        if torch.cuda.is_available():\n",
    "          model.cuda()\n",
    "        pos_u_all, pos_v_all, neg_v_all = self.make_all_word_pairs()\n",
    "        N =  len(pos_u_all)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.2) # choose your own optimization function\n",
    "        batch_num = 5000 # int(N/self.batch_size)\n",
    "        for epoch in range(self.epoch_num):    \n",
    "            for i in range(batch_num):\n",
    "                pos_u, pos_v, neg_v = pos_u_all[i*self.batch_size:(i+1)*self.batch_size], \\\n",
    "                                        pos_v_all[i*self.batch_size:(i+1)*self.batch_size], \\\n",
    "                                        neg_v_all[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        \n",
    "                pos_u = Variable(torch.LongTensor(pos_u))\n",
    "                pos_v = Variable(torch.LongTensor(pos_v))\n",
    "                neg_v = Variable(torch.LongTensor(neg_v))\n",
    "        \n",
    "                if torch.cuda.is_available():\n",
    "                  pos_u = pos_u.cuda()\n",
    "                  pos_v = pos_v.cuda()\n",
    "                  neg_v = neg_v.cuda()\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                loss = model(pos_u, pos_v, neg_v, self.batch_size)\n",
    "        \n",
    "                loss.backward()\n",
    "           \n",
    "                optimizer.step()\n",
    "            print('Epoch:', epoch)\n",
    "            print('Loss:', loss.data[0])\n",
    "        model.save_word_vectors('word2vec.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
